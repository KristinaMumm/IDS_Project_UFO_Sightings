{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import reverse_geocoder as rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kristina\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (5,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/ufo_sightings_scrubbed.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9j3ZakrST4GJ"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/ufo_sightings_scrubbed.csv\", dtype={\"duration (seconds)\": \"string\", \"latitude\": \"string\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were warning while importing 'ufo_sightings_scrubbed.csv': `DtypeWarning: Columns (5,9) have mixed types.`. So I watched the columns \"duration (seconds)\" and \"latitude\". Found out that there are values 2\\`, 8\\`, 0.5\\` in column \"duration (seconds)\" and value 33q.200088. \n",
    "\n",
    "The mark \\` after numbers seems to be mark of minutes. Removed them. Changed column dtype to numeric(float64).\n",
    "\n",
    "Value 33q.200088 should be 33.200088. I found the row with this value and Mescalero Indian Reservation location latitude is about the same as 33.200088. Changed column dtype to numeric(float64).\n",
    "\n",
    "Also working with columns the \"longitude\" was actually \"longitude \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration (seconds)\n",
      "2`\n",
      "8`\n",
      "0.5`\n",
      "latitude\n",
      "33q.200088\n"
     ]
    }
   ],
   "source": [
    "print('duration (seconds)')\n",
    "for value in data['duration (seconds)']:\n",
    "    try:\n",
    "        float(value)\n",
    "    except:\n",
    "        print(value)\n",
    "        \n",
    "print('latitude')\n",
    "for value in data['latitude']:\n",
    "    try:\n",
    "        float(value)\n",
    "    except:\n",
    "        print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace('33q.200088', '33.200088')\n",
    "data[\"latitude\"] = pd.to_numeric(data[\"latitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace('2`', '2')\n",
    "data = data.replace('8`', '8')\n",
    "data = data.replace('0.5`', '0.5')\n",
    "data[\"duration (seconds)\"] = pd.to_numeric(data[\"duration (seconds)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={'longitude ': 'longitude'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now need to decide, what to do with `null` values.\n",
    "\n",
    "Since there is no null values in \"latitude\" and \"longitude\" columns it is possible to find values for columns \"state\" and \"country\". Found that there is [\"Reverse Geocoder\" Python library](https://github.com/thampiman/reverse-geocoder) where I give coordinates and get data about the place.\n",
    "\n",
    "I checked with Reverse Geocoder, if there are any values from \"country\" that does not correspond to given coordinates with code:\n",
    "```\n",
    "from collections import Counter\n",
    "\n",
    "coordinates = []\n",
    "\n",
    "for i in data[data['country'].notnull()].index:\n",
    "    coordinates.append((data.at[i, 'latitude'], data.at[i, 'longitude']))\n",
    "\n",
    "results = rg.search(coordinates)  \n",
    "\n",
    "result_i = 0\n",
    "count = 0\n",
    "wrong = []\n",
    "for i in data[data['country'].notnull()].index:\n",
    "    if (results[result_i].get('cc').lower() != data.at[i, 'country'].lower()):\n",
    "        print('Koordinaatide järgi:', (results[result_i].get('cc').lower()))\n",
    "        print('Andmetes märgitud:',data.at[i, 'country'])\n",
    "        print(data.iloc[i])\n",
    "        print(i)\n",
    "        wrong.extend([data.at[i, 'country'] + '_' + results[result_i].get('cc').lower()]) \n",
    "        count += 1\n",
    "    result_i += 1\n",
    "    \n",
    "Counter(wrong)\n",
    "```\n",
    "The code looked about 70,000 rows and found only 93 not correspondending places. I chekced some not correspondending places manually and came out that actually the country in data is correct, but the places are so near to country borders that sometimes Reverse Geocoder thought it should be neighbour country. For example Curlew, Washington is in US, but when I gave coordinates to Reverse Geocoder, it thought it has to be Canada. Maybe because Curlew is so small place thath Reverse Geocoder does not it in its database. Result of not correspondending places, where on first place is abbreviation of country in our data and second is from Reverse Geocoder {'us_pr': 25, 'us_ca': 15, 'ca_us': 45, 'us_ru': 3, 'us_mx': 5}.  us - USA, pr - Puerto Rico, ca - Canada, ru - Russia, mx - Mexico.\n",
    "It means that I can trust the marked countries in our data and I can use Reverse Geocoder for NaN values. There could be some mistakes, but the amount of them can not be big. Also I trust now values of \"state\" columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 80332 entries, 0 to 80331\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   datetime              80332 non-null  object \n",
      " 1   city                  80332 non-null  object \n",
      " 2   state                 74535 non-null  object \n",
      " 3   country               70662 non-null  object \n",
      " 4   shape                 78400 non-null  object \n",
      " 5   duration (seconds)    80332 non-null  float64\n",
      " 6   duration (hours/min)  80332 non-null  object \n",
      " 7   comments              80317 non-null  object \n",
      " 8   date posted           80332 non-null  object \n",
      " 9   latitude              80332 non-null  float64\n",
      " 10  longitude             80332 non-null  float64\n",
      "dtypes: float64(3), object(8)\n",
      "memory usage: 6.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = {\n",
    "    'Alaska': 'ak',\n",
    "    'Alabama': 'al',\n",
    "    'Arkansas': 'ar',\n",
    "    'Arizona': 'az',\n",
    "    'California': 'ca',\n",
    "    'Colorado': 'co',\n",
    "    'Connecticut': 'ct',\n",
    "    'District of Columbia': 'dc',\n",
    "    'Delaware': 'de',\n",
    "    'Florida': 'fl',\n",
    "    'Georgia': 'ga',\n",
    "    'Hawaii': 'hi',\n",
    "    'Iowa': 'ia',\n",
    "    'Idaho': 'id',\n",
    "    'Illinois': 'il',\n",
    "    'Indiana': 'in',\n",
    "    'Kansas': 'ks',\n",
    "    'Kentucky': 'ky',\n",
    "    'Louisiana': 'la',\n",
    "    'Massachusetts': 'ma',\n",
    "    'Maryland': 'md',\n",
    "    'Maine': 'me',\n",
    "    'Michigan': 'mi',\n",
    "    'Minnesota': 'mn',\n",
    "    'Missouri': 'mo',\n",
    "    'Mississippi': 'ms',\n",
    "    'Montana': 'mt',\n",
    "    'North Carolina': 'nc',\n",
    "    'North Dakota': 'nd',\n",
    "    'Nebraska': 'ne',\n",
    "    'New Hampshire': 'nh',\n",
    "    'New Jersey': 'nj',\n",
    "    'New Mexico': 'nm',\n",
    "    'Nevada': 'nv',\n",
    "    'New York': 'ny',\n",
    "    'Ohio': 'oh',\n",
    "    'Oklahoma': 'ok',\n",
    "    'Oregon': 'or',\n",
    "    'Pennsylvania': 'pa',\n",
    "    'Rhode Island': 'ri',\n",
    "    'South Carolina': 'sc',\n",
    "    'South Dakota': 'sd',\n",
    "    'Tennessee': 'tn',\n",
    "    'Texas': 'tx',\n",
    "    'Utah': 'ut',\n",
    "    'Virginia': 'va',\n",
    "    'Vermont': 'vt',\n",
    "    'Washington': 'wa',\n",
    "    'Wisconsin': 'wi',\n",
    "    'West Virginia': 'wv',\n",
    "    'Wyoming': 'wy'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading formatted geocoded file...\n"
     ]
    }
   ],
   "source": [
    "#Get all rows where country or state is null. Source: https://stackoverflow.com/questions/23330654/update-a-dataframe-in-pandas-while-iterating-row-by-row\n",
    "#Fill state onyl when it is US?\n",
    "coordinates = []\n",
    "result_i = 0\n",
    "for i in data[data['country'].isnull() |  ((data['state'].isna()) & (data['country'] == 'us'))].index:\n",
    "    \n",
    "    coordinates.append((data.at[i, 'latitude'], data.at[i, 'longitude']))\n",
    "    \n",
    "results = rg.search(coordinates)\n",
    "\n",
    "for i in data[data['country'].isnull() |  ((data['state'].isna()) & (data['country'] == 'us'))].index:\n",
    "    if pd.isna(data.at[i, 'country']):\n",
    "        data.at[i, 'country'] = results[0].get('cc').lower()\n",
    "        \n",
    "    if (pd.isna(data.at[i, 'state'])) & (data.at[i, 'country'] == 'us'):\n",
    "        data.at[i, 'country'] = states.get(results[0].get('admin1'))\n",
    "        \n",
    "    result_i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now need to do something with shape values. Since the value \"light\" is most represented, 16,565 times and second most represented value is \"triangle\", 7865 times, then it seems to okay use \"light\" value for ~2000 NaN values. Also some shapes should be renamed like \"round\" to \"circle\" , because the meaning of them is same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['shape'] = data['shape'].replace(np.nan, 'light')\n",
    "data['shape'] = data['shape'].replace('round', 'circle')\n",
    "data['shape'] = data['shape'].replace('changed', 'changing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving to new .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('ufo_sightings_cleaned.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = pd.read_csv(\"data/ufo_sightings_scrubbed.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "UFO_sightings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
